{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D, Dense\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset = 'data/fer2013.csv'\n",
    "image_size = (48,48)\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv(dataset)\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'),image_size)\n",
    "        faces.append(face.astype('float32'))\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
    "    return faces, emotions\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    "\n",
    "faces, emotions = load_data()\n",
    "faces = preprocess_input(faces)\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 44, 44, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 18, 18, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              5185000   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 7007      \n",
      "=================================================================\n",
      "Total params: 5,244,103\n",
      "Trainable params: 5,244,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "input_shape = (48, 48, 1)\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = 'models/'\n",
    "l2_regularization=0.01\n",
    " \n",
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "898/897 [==============================] - 16s 18ms/step - loss: 1.6446 - acc: 0.3435 - val_loss: 1.4189 - val_acc: 0.4532\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.41890, saving model to models/_CNN_model_.01-0.45.hdf5\n",
      "Epoch 2/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.4434 - acc: 0.4421 - val_loss: 1.3262 - val_acc: 0.4939\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.41890 to 1.32625, saving model to models/_CNN_model_.02-0.49.hdf5\n",
      "Epoch 3/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.3532 - acc: 0.4786 - val_loss: 1.2421 - val_acc: 0.5259\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.32625 to 1.24209, saving model to models/_CNN_model_.03-0.53.hdf5\n",
      "Epoch 4/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.2995 - acc: 0.5023 - val_loss: 1.2268 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.24209 to 1.22678, saving model to models/_CNN_model_.04-0.54.hdf5\n",
      "Epoch 5/50\n",
      "898/897 [==============================] - 17s 19ms/step - loss: 1.2647 - acc: 0.5179 - val_loss: 1.2086 - val_acc: 0.5361\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.22678 to 1.20856, saving model to models/_CNN_model_.05-0.54.hdf5\n",
      "Epoch 6/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.2362 - acc: 0.5251 - val_loss: 1.1580 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.20856 to 1.15801, saving model to models/_CNN_model_.06-0.57.hdf5\n",
      "Epoch 7/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 1.2057 - acc: 0.5381 - val_loss: 1.1808 - val_acc: 0.5496\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.15801\n",
      "Epoch 8/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 1.1941 - acc: 0.5448 - val_loss: 1.1227 - val_acc: 0.5751\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.15801 to 1.12272, saving model to models/_CNN_model_.08-0.58.hdf5\n",
      "Epoch 9/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 1.1737 - acc: 0.5549 - val_loss: 1.1632 - val_acc: 0.5598\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.12272\n",
      "Epoch 10/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.1551 - acc: 0.5620 - val_loss: 1.1283 - val_acc: 0.5816\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.12272\n",
      "Epoch 11/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.1444 - acc: 0.5637 - val_loss: 1.1300 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.12272\n",
      "Epoch 12/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.1322 - acc: 0.5689 - val_loss: 1.1177 - val_acc: 0.5790\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.12272 to 1.11769, saving model to models/_CNN_model_.12-0.58.hdf5\n",
      "Epoch 13/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 1.1153 - acc: 0.5767 - val_loss: 1.1001 - val_acc: 0.5862\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.11769 to 1.10015, saving model to models/_CNN_model_.13-0.59.hdf5\n",
      "Epoch 14/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.1093 - acc: 0.5810 - val_loss: 1.1522 - val_acc: 0.5727\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.10015\n",
      "Epoch 15/50\n",
      "898/897 [==============================] - 16s 18ms/step - loss: 1.0981 - acc: 0.5833 - val_loss: 1.1169 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.10015\n",
      "Epoch 16/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 1.0919 - acc: 0.5859 - val_loss: 1.0981 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.10015 to 1.09808, saving model to models/_CNN_model_.16-0.59.hdf5\n",
      "Epoch 17/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 1.0836 - acc: 0.5897 - val_loss: 1.0831 - val_acc: 0.5935\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.09808 to 1.08306, saving model to models/_CNN_model_.17-0.59.hdf5\n",
      "Epoch 18/50\n",
      "898/897 [==============================] - 18s 20ms/step - loss: 1.0740 - acc: 0.5925 - val_loss: 1.1057 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.08306\n",
      "Epoch 19/50\n",
      "898/897 [==============================] - 16s 17ms/step - loss: 1.0619 - acc: 0.5971 - val_loss: 1.0960 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.08306\n",
      "Epoch 20/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.0627 - acc: 0.6001 - val_loss: 1.0881 - val_acc: 0.5978\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.08306\n",
      "Epoch 21/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.0431 - acc: 0.6079 - val_loss: 1.0981 - val_acc: 0.6004\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.08306\n",
      "Epoch 22/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.0375 - acc: 0.6102 - val_loss: 1.1058 - val_acc: 0.5943\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.08306\n",
      "Epoch 23/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.0338 - acc: 0.6106 - val_loss: 1.1115 - val_acc: 0.5963\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.08306\n",
      "Epoch 24/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.0284 - acc: 0.6135 - val_loss: 1.0980 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.08306\n",
      "Epoch 25/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 1.0184 - acc: 0.6159 - val_loss: 1.0859 - val_acc: 0.6030\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.08306\n",
      "Epoch 26/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.0126 - acc: 0.6203 - val_loss: 1.1018 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.08306\n",
      "Epoch 27/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.0023 - acc: 0.6227 - val_loss: 1.0967 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.08306\n",
      "Epoch 28/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 1.0101 - acc: 0.6212 - val_loss: 1.1066 - val_acc: 0.6048\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.08306\n",
      "Epoch 29/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 1.0029 - acc: 0.6267 - val_loss: 1.1105 - val_acc: 0.6041\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.08306\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 30/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 0.9369 - acc: 0.6512 - val_loss: 1.0649 - val_acc: 0.6259\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.08306 to 1.06494, saving model to models/_CNN_model_.30-0.63.hdf5\n",
      "Epoch 31/50\n",
      "898/897 [==============================] - 16s 17ms/step - loss: 0.9222 - acc: 0.6552 - val_loss: 1.0635 - val_acc: 0.6265\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.06494 to 1.06352, saving model to models/_CNN_model_.31-0.63.hdf5\n",
      "Epoch 32/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.9113 - acc: 0.6610 - val_loss: 1.0542 - val_acc: 0.6287\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.06352 to 1.05417, saving model to models/_CNN_model_.32-0.63.hdf5\n",
      "Epoch 33/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 0.9062 - acc: 0.6612 - val_loss: 1.0534 - val_acc: 0.6289\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.05417 to 1.05343, saving model to models/_CNN_model_.33-0.63.hdf5\n",
      "Epoch 34/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 0.8940 - acc: 0.6639 - val_loss: 1.0557 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.05343\n",
      "Epoch 35/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 0.8938 - acc: 0.6682 - val_loss: 1.0554 - val_acc: 0.6330\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.05343\n",
      "Epoch 36/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8934 - acc: 0.6670 - val_loss: 1.0545 - val_acc: 0.6296\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.05343\n",
      "Epoch 37/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 0.8804 - acc: 0.6746 - val_loss: 1.0597 - val_acc: 0.6303\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.05343\n",
      "Epoch 38/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 0.8852 - acc: 0.6686 - val_loss: 1.0585 - val_acc: 0.6321\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.05343\n",
      "Epoch 39/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 0.8802 - acc: 0.6712 - val_loss: 1.0571 - val_acc: 0.6329\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.05343\n",
      "Epoch 40/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 0.8800 - acc: 0.6731 - val_loss: 1.0532 - val_acc: 0.6311\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.05343 to 1.05324, saving model to models/_CNN_model_.40-0.63.hdf5\n",
      "Epoch 41/50\n",
      "898/897 [==============================] - 14s 16ms/step - loss: 0.8732 - acc: 0.6749 - val_loss: 1.0566 - val_acc: 0.6286\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.05324\n",
      "Epoch 42/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8782 - acc: 0.6713 - val_loss: 1.0566 - val_acc: 0.6290\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.05324\n",
      "Epoch 43/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8718 - acc: 0.6734 - val_loss: 1.0495 - val_acc: 0.6322\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.05324 to 1.04954, saving model to models/_CNN_model_.43-0.63.hdf5\n",
      "Epoch 44/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8679 - acc: 0.6781 - val_loss: 1.0490 - val_acc: 0.6272\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.04954 to 1.04897, saving model to models/_CNN_model_.44-0.63.hdf5\n",
      "Epoch 45/50\n",
      "898/897 [==============================] - 15s 17ms/step - loss: 0.8664 - acc: 0.6777 - val_loss: 1.0512 - val_acc: 0.6367\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.04897\n",
      "Epoch 46/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8571 - acc: 0.6806 - val_loss: 1.0459 - val_acc: 0.6347\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.04897 to 1.04594, saving model to models/_CNN_model_.46-0.63.hdf5\n",
      "Epoch 47/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8602 - acc: 0.6816 - val_loss: 1.0566 - val_acc: 0.6293\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.04594\n",
      "Epoch 48/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8596 - acc: 0.6766 - val_loss: 1.0481 - val_acc: 0.6328\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.04594\n",
      "Epoch 49/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8481 - acc: 0.6868 - val_loss: 1.0519 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.04594\n",
      "Epoch 50/50\n",
      "898/897 [==============================] - 15s 16ms/step - loss: 0.8525 - acc: 0.6813 - val_loss: 1.0523 - val_acc: 0.6317\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.04594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24f96bbb128>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_file_path = base_path + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + '_CNN_model_'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "\n",
    "faces, emotions = load_data()\n",
    "faces = preprocess_input(faces)\n",
    "num_samples, num_classes = emotions.shape\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)\n",
    "model.fit_generator(data_generator.flow(xtrain, ytrain,\n",
    "                                            batch_size),\n",
    "                        steps_per_epoch=len(xtrain) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_files/haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "model = load_model('models/_CNN_model_.46-0.63.hdf5')\n",
    "\n",
    "target = ['angry','disgust','fear','happy','sad','surprise','neutral']\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1)\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2,5)\n",
    "        face_crop = frame[y:y+h,x:x+w]\n",
    "        face_crop = cv2.resize(face_crop,(48,48))\n",
    "        face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "        face_crop = face_crop.astype('float32')/255\n",
    "        face_crop = np.asarray(face_crop)\n",
    "        face_crop = face_crop.reshape(1, face_crop.shape[0],face_crop.shape[1],1)\n",
    "        result = target[np.argmax(model.predict(face_crop))]\n",
    "        cv2.putText(frame,result,(x,y), font, 1, (255,255,255), 3, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>epoch</td>\n",
       "      <td>acc</td>\n",
       "      <td>loss</td>\n",
       "      <td>val_acc</td>\n",
       "      <td>val_loss\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.34658121146730647</td>\n",
       "      <td>1.6297167692693026</td>\n",
       "      <td>0.4600167283422437</td>\n",
       "      <td>1.4280669269525397\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.450207252082267</td>\n",
       "      <td>1.4145038691329053</td>\n",
       "      <td>0.5114238062490989</td>\n",
       "      <td>1.2731193671125984\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.48880142115764746</td>\n",
       "      <td>1.3314379798178015</td>\n",
       "      <td>0.5339927676827438</td>\n",
       "      <td>1.2302504519120536\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5095266292805357</td>\n",
       "      <td>1.2820175922237937</td>\n",
       "      <td>0.5231262306527329</td>\n",
       "      <td>1.2427245141836087\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5243651816524297</td>\n",
       "      <td>1.248310371660888</td>\n",
       "      <td>0.5493173708230481</td>\n",
       "      <td>1.185818754108537\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5337002333804455</td>\n",
       "      <td>1.2211829432042989</td>\n",
       "      <td>0.5647812887972263</td>\n",
       "      <td>1.1619964280926307\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>0.5410846772809782</td>\n",
       "      <td>1.2030045355249503</td>\n",
       "      <td>0.5565617286493002</td>\n",
       "      <td>1.1619291304753132\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>0.5513950329175451</td>\n",
       "      <td>1.175949544761949</td>\n",
       "      <td>0.5697966132984582</td>\n",
       "      <td>1.130658319904491\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0.5582569925812428</td>\n",
       "      <td>1.1650808358243103</td>\n",
       "      <td>0.5770409709378749</td>\n",
       "      <td>1.1171059515000252\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>0.5570726949747397</td>\n",
       "      <td>1.1512141815235575</td>\n",
       "      <td>0.5799665774461884</td>\n",
       "      <td>1.115657874905393\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5684280190886098</td>\n",
       "      <td>1.1343095823118083</td>\n",
       "      <td>0.5870716202517312</td>\n",
       "      <td>1.102654362571218\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>0.572956215824975</td>\n",
       "      <td>1.12141206483822</td>\n",
       "      <td>0.5873502493382131</td>\n",
       "      <td>1.1091920498091137\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>0.57633494723013</td>\n",
       "      <td>1.115268316134053</td>\n",
       "      <td>0.5881861371996842</td>\n",
       "      <td>1.1092179323491835\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>0.581594621897649</td>\n",
       "      <td>1.0998455963416627</td>\n",
       "      <td>0.5799665774150491</td>\n",
       "      <td>1.1054357299677182\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>0.5838587202629768</td>\n",
       "      <td>1.0967349966158084</td>\n",
       "      <td>0.5959877531677609</td>\n",
       "      <td>1.0880407636964942\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>0.5848688564572017</td>\n",
       "      <td>1.0894987978130188</td>\n",
       "      <td>0.5865143622967419</td>\n",
       "      <td>1.1168612995267737\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>0.5884914138433872</td>\n",
       "      <td>1.0821090107024902</td>\n",
       "      <td>0.5991919880982527</td>\n",
       "      <td>1.0873637448764837\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>0.5905465185175105</td>\n",
       "      <td>1.0770726970065172</td>\n",
       "      <td>0.5986347301121242</td>\n",
       "      <td>1.0661425286056472\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>0.5951443798132852</td>\n",
       "      <td>1.0679343437412705</td>\n",
       "      <td>0.5954304949844172</td>\n",
       "      <td>1.0976865499460222\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>0.6009613709996796</td>\n",
       "      <td>1.0591168699686735</td>\n",
       "      <td>0.6090833230660831</td>\n",
       "      <td>1.0726186687414274\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>0.5960848514423911</td>\n",
       "      <td>1.060357572824015</td>\n",
       "      <td>0.5991919882850881</td>\n",
       "      <td>1.0824054151002238\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>0.6003343899167233</td>\n",
       "      <td>1.0480559357339807</td>\n",
       "      <td>0.5982167864201228</td>\n",
       "      <td>1.0839289218606674\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>0.6038176181690377</td>\n",
       "      <td>1.044924955909672</td>\n",
       "      <td>0.5957091240086207</td>\n",
       "      <td>1.1037099245836033\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>0.6120032045741478</td>\n",
       "      <td>1.0369075282314544</td>\n",
       "      <td>0.5972415842645246</td>\n",
       "      <td>1.0836390113963013\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>0.610435751854819</td>\n",
       "      <td>1.0284006526412965</td>\n",
       "      <td>0.5979381573336409</td>\n",
       "      <td>1.0905885986436763\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>0.6098784353359435</td>\n",
       "      <td>1.0281706839429285</td>\n",
       "      <td>0.6029534819698095</td>\n",
       "      <td>1.078129434157822\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>0.6126650179427778</td>\n",
       "      <td>1.0266026243105</td>\n",
       "      <td>0.6125661875916644</td>\n",
       "      <td>1.0738508474968185\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>0.6133616635908531</td>\n",
       "      <td>1.0214177185043511</td>\n",
       "      <td>0.6005851341638269</td>\n",
       "      <td>1.0927586450687057\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>0.6114110557664844</td>\n",
       "      <td>1.0220879288156386</td>\n",
       "      <td>0.6075508625818249</td>\n",
       "      <td>1.0731522238671398\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>60</td>\n",
       "      <td>0.6782193737197119</td>\n",
       "      <td>0.853072151129072</td>\n",
       "      <td>0.6276121613756134</td>\n",
       "      <td>1.0501412768788816\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>61</td>\n",
       "      <td>0.6784631996976282</td>\n",
       "      <td>0.8527506654665901</td>\n",
       "      <td>0.6278907905347535</td>\n",
       "      <td>1.0508297840955905\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>62</td>\n",
       "      <td>0.6785676965421924</td>\n",
       "      <td>0.8507973034224326</td>\n",
       "      <td>0.6260797011716082</td>\n",
       "      <td>1.0518010613220745\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>63</td>\n",
       "      <td>0.6772440698080466</td>\n",
       "      <td>0.8528734705424814</td>\n",
       "      <td>0.6270549031507507</td>\n",
       "      <td>1.0515853958305592\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>64</td>\n",
       "      <td>0.6769305792629351</td>\n",
       "      <td>0.8504314338719942</td>\n",
       "      <td>0.6266369594276101</td>\n",
       "      <td>1.0515746679779803\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>65</td>\n",
       "      <td>0.6753631265498349</td>\n",
       "      <td>0.8577678145408597</td>\n",
       "      <td>0.625940386597228</td>\n",
       "      <td>1.0515932002005952\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>66</td>\n",
       "      <td>0.6768609146967781</td>\n",
       "      <td>0.85147865327898</td>\n",
       "      <td>0.6258010720228477</td>\n",
       "      <td>1.051505406984764\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>67</td>\n",
       "      <td>0.6753979588323944</td>\n",
       "      <td>0.8554401915251696</td>\n",
       "      <td>0.6263583303203687</td>\n",
       "      <td>1.0515102850427391\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>68</td>\n",
       "      <td>0.678254205998119</td>\n",
       "      <td>0.8528196734162374</td>\n",
       "      <td>0.6262190157044695</td>\n",
       "      <td>1.0513125422570775\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>69</td>\n",
       "      <td>0.680274478390721</td>\n",
       "      <td>0.8492720691199969</td>\n",
       "      <td>0.6264976448739894</td>\n",
       "      <td>1.0510735812849463\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>70</td>\n",
       "      <td>0.6792295099139367</td>\n",
       "      <td>0.850868981548676</td>\n",
       "      <td>0.6263583302788497</td>\n",
       "      <td>1.0512372915975294\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>71</td>\n",
       "      <td>0.6790901807836988</td>\n",
       "      <td>0.849627580690535</td>\n",
       "      <td>0.6262190157148492</td>\n",
       "      <td>1.051112150089381\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>72</td>\n",
       "      <td>0.677592392630008</td>\n",
       "      <td>0.8530984683828903</td>\n",
       "      <td>0.6266369594483696</td>\n",
       "      <td>1.0508644568131067\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>73</td>\n",
       "      <td>0.6766519210014212</td>\n",
       "      <td>0.8534104596476864</td>\n",
       "      <td>0.62677627401237</td>\n",
       "      <td>1.0507842272802395\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>74</td>\n",
       "      <td>0.677278902090606</td>\n",
       "      <td>0.8584415781331447</td>\n",
       "      <td>0.6267762740019903</td>\n",
       "      <td>1.0505265363191838\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>75</td>\n",
       "      <td>0.6768957469824518</td>\n",
       "      <td>0.8503467891091625</td>\n",
       "      <td>0.6271942177043714</td>\n",
       "      <td>1.0504547316439847\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>76</td>\n",
       "      <td>0.6755721202420775</td>\n",
       "      <td>0.8487614463740845</td>\n",
       "      <td>0.6271942177043714</td>\n",
       "      <td>1.0504573598326572\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>77</td>\n",
       "      <td>0.6770350761090564</td>\n",
       "      <td>0.8517839735115437</td>\n",
       "      <td>0.6271942177043714</td>\n",
       "      <td>1.050458455397562\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>78</td>\n",
       "      <td>0.6768957469824518</td>\n",
       "      <td>0.8548378153668456</td>\n",
       "      <td>0.6271942177043714</td>\n",
       "      <td>1.0504520737401795\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>79</td>\n",
       "      <td>0.6763035981768646</td>\n",
       "      <td>0.8559246233394974</td>\n",
       "      <td>0.6271942177043714</td>\n",
       "      <td>1.0504451178441925\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>80</td>\n",
       "      <td>0.6757114493733535</td>\n",
       "      <td>0.8552686611148331</td>\n",
       "      <td>0.6271942177043714</td>\n",
       "      <td>1.0504199388219257\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>81</td>\n",
       "      <td>0.6742484935058555</td>\n",
       "      <td>0.8571216241498448</td>\n",
       "      <td>0.6270549031299912</td>\n",
       "      <td>1.0504525153708013\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>82</td>\n",
       "      <td>0.6776272249130866</td>\n",
       "      <td>0.8526784332366011</td>\n",
       "      <td>0.626915588555611</td>\n",
       "      <td>1.0504832507647506\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>83</td>\n",
       "      <td>0.6786025288278662</td>\n",
       "      <td>0.8521247431088268</td>\n",
       "      <td>0.6270549031196115</td>\n",
       "      <td>1.050481608189906\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>84</td>\n",
       "      <td>0.679194677628263</td>\n",
       "      <td>0.852845305293459</td>\n",
       "      <td>0.6271942176939918</td>\n",
       "      <td>1.0504721509597368\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>85</td>\n",
       "      <td>0.677801386326922</td>\n",
       "      <td>0.8490947459578585</td>\n",
       "      <td>0.6270549031196115</td>\n",
       "      <td>1.0505040264040995\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>86</td>\n",
       "      <td>0.679125013063144</td>\n",
       "      <td>0.8448582787453773</td>\n",
       "      <td>0.6270549031196115</td>\n",
       "      <td>1.0504988705935037\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>87</td>\n",
       "      <td>0.6783238705653142</td>\n",
       "      <td>0.8543828597923004</td>\n",
       "      <td>0.6271942176939918</td>\n",
       "      <td>1.0504663727573447\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>88</td>\n",
       "      <td>0.6775575603469295</td>\n",
       "      <td>0.8525458753416251</td>\n",
       "      <td>0.6271942176939918</td>\n",
       "      <td>1.0504644747167478\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>89</td>\n",
       "      <td>0.6759204430676723</td>\n",
       "      <td>0.8527267143815956</td>\n",
       "      <td>0.6271942176939918</td>\n",
       "      <td>1.050465670192402\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs                  acc                loss             val_acc  \\\n",
       "0   epoch                  acc                loss             val_acc   \n",
       "1       0  0.34658121146730647  1.6297167692693026  0.4600167283422437   \n",
       "2       1    0.450207252082267  1.4145038691329053  0.5114238062490989   \n",
       "3       2  0.48880142115764746  1.3314379798178015  0.5339927676827438   \n",
       "4       3   0.5095266292805357  1.2820175922237937  0.5231262306527329   \n",
       "5       4   0.5243651816524297   1.248310371660888  0.5493173708230481   \n",
       "6       5   0.5337002333804455  1.2211829432042989  0.5647812887972263   \n",
       "7       6   0.5410846772809782  1.2030045355249503  0.5565617286493002   \n",
       "8       7   0.5513950329175451   1.175949544761949  0.5697966132984582   \n",
       "9       8   0.5582569925812428  1.1650808358243103  0.5770409709378749   \n",
       "10      9   0.5570726949747397  1.1512141815235575  0.5799665774461884   \n",
       "11     10   0.5684280190886098  1.1343095823118083  0.5870716202517312   \n",
       "12     11    0.572956215824975    1.12141206483822  0.5873502493382131   \n",
       "13     12     0.57633494723013   1.115268316134053  0.5881861371996842   \n",
       "14     13    0.581594621897649  1.0998455963416627  0.5799665774150491   \n",
       "15     14   0.5838587202629768  1.0967349966158084  0.5959877531677609   \n",
       "16     15   0.5848688564572017  1.0894987978130188  0.5865143622967419   \n",
       "17     16   0.5884914138433872  1.0821090107024902  0.5991919880982527   \n",
       "18     17   0.5905465185175105  1.0770726970065172  0.5986347301121242   \n",
       "19     18   0.5951443798132852  1.0679343437412705  0.5954304949844172   \n",
       "20     19   0.6009613709996796  1.0591168699686735  0.6090833230660831   \n",
       "21     20   0.5960848514423911   1.060357572824015  0.5991919882850881   \n",
       "22     21   0.6003343899167233  1.0480559357339807  0.5982167864201228   \n",
       "23     22   0.6038176181690377   1.044924955909672  0.5957091240086207   \n",
       "24     23   0.6120032045741478  1.0369075282314544  0.5972415842645246   \n",
       "25     24    0.610435751854819  1.0284006526412965  0.5979381573336409   \n",
       "26     25   0.6098784353359435  1.0281706839429285  0.6029534819698095   \n",
       "27     26   0.6126650179427778     1.0266026243105  0.6125661875916644   \n",
       "28     27   0.6133616635908531  1.0214177185043511  0.6005851341638269   \n",
       "29     28   0.6114110557664844  1.0220879288156386  0.6075508625818249   \n",
       "..    ...                  ...                 ...                 ...   \n",
       "61     60   0.6782193737197119   0.853072151129072  0.6276121613756134   \n",
       "62     61   0.6784631996976282  0.8527506654665901  0.6278907905347535   \n",
       "63     62   0.6785676965421924  0.8507973034224326  0.6260797011716082   \n",
       "64     63   0.6772440698080466  0.8528734705424814  0.6270549031507507   \n",
       "65     64   0.6769305792629351  0.8504314338719942  0.6266369594276101   \n",
       "66     65   0.6753631265498349  0.8577678145408597   0.625940386597228   \n",
       "67     66   0.6768609146967781    0.85147865327898  0.6258010720228477   \n",
       "68     67   0.6753979588323944  0.8554401915251696  0.6263583303203687   \n",
       "69     68    0.678254205998119  0.8528196734162374  0.6262190157044695   \n",
       "70     69    0.680274478390721  0.8492720691199969  0.6264976448739894   \n",
       "71     70   0.6792295099139367   0.850868981548676  0.6263583302788497   \n",
       "72     71   0.6790901807836988   0.849627580690535  0.6262190157148492   \n",
       "73     72    0.677592392630008  0.8530984683828903  0.6266369594483696   \n",
       "74     73   0.6766519210014212  0.8534104596476864    0.62677627401237   \n",
       "75     74    0.677278902090606  0.8584415781331447  0.6267762740019903   \n",
       "76     75   0.6768957469824518  0.8503467891091625  0.6271942177043714   \n",
       "77     76   0.6755721202420775  0.8487614463740845  0.6271942177043714   \n",
       "78     77   0.6770350761090564  0.8517839735115437  0.6271942177043714   \n",
       "79     78   0.6768957469824518  0.8548378153668456  0.6271942177043714   \n",
       "80     79   0.6763035981768646  0.8559246233394974  0.6271942177043714   \n",
       "81     80   0.6757114493733535  0.8552686611148331  0.6271942177043714   \n",
       "82     81   0.6742484935058555  0.8571216241498448  0.6270549031299912   \n",
       "83     82   0.6776272249130866  0.8526784332366011   0.626915588555611   \n",
       "84     83   0.6786025288278662  0.8521247431088268  0.6270549031196115   \n",
       "85     84    0.679194677628263   0.852845305293459  0.6271942176939918   \n",
       "86     85    0.677801386326922  0.8490947459578585  0.6270549031196115   \n",
       "87     86    0.679125013063144  0.8448582787453773  0.6270549031196115   \n",
       "88     87   0.6783238705653142  0.8543828597923004  0.6271942176939918   \n",
       "89     88   0.6775575603469295  0.8525458753416251  0.6271942176939918   \n",
       "90     89   0.6759204430676723  0.8527267143815956  0.6271942176939918   \n",
       "\n",
       "                val_loss  \n",
       "0             val_loss\\n  \n",
       "1   1.4280669269525397\\n  \n",
       "2   1.2731193671125984\\n  \n",
       "3   1.2302504519120536\\n  \n",
       "4   1.2427245141836087\\n  \n",
       "5    1.185818754108537\\n  \n",
       "6   1.1619964280926307\\n  \n",
       "7   1.1619291304753132\\n  \n",
       "8    1.130658319904491\\n  \n",
       "9   1.1171059515000252\\n  \n",
       "10   1.115657874905393\\n  \n",
       "11   1.102654362571218\\n  \n",
       "12  1.1091920498091137\\n  \n",
       "13  1.1092179323491835\\n  \n",
       "14  1.1054357299677182\\n  \n",
       "15  1.0880407636964942\\n  \n",
       "16  1.1168612995267737\\n  \n",
       "17  1.0873637448764837\\n  \n",
       "18  1.0661425286056472\\n  \n",
       "19  1.0976865499460222\\n  \n",
       "20  1.0726186687414274\\n  \n",
       "21  1.0824054151002238\\n  \n",
       "22  1.0839289218606674\\n  \n",
       "23  1.1037099245836033\\n  \n",
       "24  1.0836390113963013\\n  \n",
       "25  1.0905885986436763\\n  \n",
       "26   1.078129434157822\\n  \n",
       "27  1.0738508474968185\\n  \n",
       "28  1.0927586450687057\\n  \n",
       "29  1.0731522238671398\\n  \n",
       "..                   ...  \n",
       "61  1.0501412768788816\\n  \n",
       "62  1.0508297840955905\\n  \n",
       "63  1.0518010613220745\\n  \n",
       "64  1.0515853958305592\\n  \n",
       "65  1.0515746679779803\\n  \n",
       "66  1.0515932002005952\\n  \n",
       "67   1.051505406984764\\n  \n",
       "68  1.0515102850427391\\n  \n",
       "69  1.0513125422570775\\n  \n",
       "70  1.0510735812849463\\n  \n",
       "71  1.0512372915975294\\n  \n",
       "72   1.051112150089381\\n  \n",
       "73  1.0508644568131067\\n  \n",
       "74  1.0507842272802395\\n  \n",
       "75  1.0505265363191838\\n  \n",
       "76  1.0504547316439847\\n  \n",
       "77  1.0504573598326572\\n  \n",
       "78   1.050458455397562\\n  \n",
       "79  1.0504520737401795\\n  \n",
       "80  1.0504451178441925\\n  \n",
       "81  1.0504199388219257\\n  \n",
       "82  1.0504525153708013\\n  \n",
       "83  1.0504832507647506\\n  \n",
       "84   1.050481608189906\\n  \n",
       "85  1.0504721509597368\\n  \n",
       "86  1.0505040264040995\\n  \n",
       "87  1.0504988705935037\\n  \n",
       "88  1.0504663727573447\\n  \n",
       "89  1.0504644747167478\\n  \n",
       "90   1.050465670192402\\n  \n",
       "\n",
       "[91 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_data  = open('models/_emotion_training.log', 'r')\n",
    "split_list = []\n",
    "\n",
    "for line in log_data:\n",
    "    data = line.split(',')\n",
    "    #print(thing1)\n",
    "    epochs = data[0]\n",
    "    acc = data[1]\n",
    "    loss = data[2]\n",
    "    val_acc = data[3]\n",
    "    val_loss = data[4]\n",
    "\n",
    "    split_list.append([epochs, acc, loss, val_acc, val_loss])\n",
    "\n",
    "df = pd.DataFrame(split_list, columns=['epochs','acc','loss','val_acc','val_loss'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
